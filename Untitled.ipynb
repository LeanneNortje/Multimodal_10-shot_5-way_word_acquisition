{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6324431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DataParallel as DDP\n",
    "from dataloaders import *\n",
    "from models.setup import *\n",
    "from models.GeneralModels import *\n",
    "from models.multimodalModels import *\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as trainable_parameters\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import scipy\n",
    "import scipy.signal\n",
    "from scipy.spatial import distance\n",
    "import librosa\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9588ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NFrames(audio_input, audio_output, nframes, with_torch=True):\n",
    "    pooling_ratio = round(audio_input.size(-1) / audio_output.size(-1))\n",
    "    if with_torch: pooling_ratio = torch.tensor(pooling_ratio, dtype=torch.int32)\n",
    "    nframes = nframes.float()\n",
    "    nframes.div_(pooling_ratio)\n",
    "    nframes = nframes.int()\n",
    "    zeros = (nframes == 0).nonzero()\n",
    "    if zeros.nelement() != 0: nframes[zeros[:, 0]] += 1\n",
    "\n",
    "    return nframes\n",
    "\n",
    "def getParameters(models, to_freeze, args):\n",
    "    valid_models = []\n",
    "    for model_name in models:\n",
    "        valid_models.append(\n",
    "            {\n",
    "            'params': models[model_name].parameters(),\n",
    "            'lr': args[\"learning_rate_scheduler\"][\"initial_learning_rate\"],\n",
    "            'name': model_name\n",
    "            }\n",
    "            )\n",
    "\n",
    "    for model_name in to_freeze:\n",
    "        for n, p in to_freeze[model_name].named_parameters(): \n",
    "            if n.startswith('embedder'):\n",
    "                valid_models.append(\n",
    "                {\n",
    "                'params': p,\n",
    "                'lr': args[\"learning_rate_scheduler\"][\"initial_learning_rate\"],\n",
    "                'name': model_name + \"_\" + n\n",
    "                }\n",
    "                )\n",
    "\n",
    "    return valid_models \n",
    "\n",
    "def saveModelAttriburesAndTrainingAMP(\n",
    "    exp_dir, audio_model, image_model, attention, contrastive_loss, optimizer, #amp,\n",
    "    info, epoch, global_step, best_epoch, acc, best_acc, loss, epoch_time\n",
    "    ):\n",
    "    \n",
    "    overwrite_best_ckpt = False\n",
    "    if acc > best_acc:\n",
    "        best_epoch = epoch\n",
    "        best_acc = acc\n",
    "        overwrite_best_ckpt = True\n",
    "\n",
    "    # assert int(epoch) not in info\n",
    "    info[int(epoch)] = {\n",
    "        \"global_step\": global_step,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"acc\": acc,\n",
    "        \"best_acc\": best_acc,\n",
    "        \"loss\": loss,\n",
    "        \"epoch_time\": epoch_time\n",
    "    }\n",
    "    with open(exp_dir / \"training_metadata.json\", \"w\") as f:\n",
    "        json.dump(info, f)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"audio_model\": audio_model.state_dict(),\n",
    "        \"image_model\": image_model.state_dict(),\n",
    "        \"attention\": attention.state_dict(),\n",
    "        \"contrastive_loss\": contrastive_loss.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        # \"amp\": amp.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"acc\": acc,\n",
    "        \"best_acc\": best_acc,\n",
    "        \"loss\": loss,\n",
    "        \"epoch_time\": epoch_time\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir(exp_dir / \"models\"): os.makedirs(exp_dir / \"models\")\n",
    "    torch.save(checkpoint, exp_dir / \"models\" / \"last_ckpt.pt\")\n",
    "    if overwrite_best_ckpt:\n",
    "        torch.save(checkpoint, exp_dir / \"models\" / \"best_ckpt.pt\")\n",
    "    torch.save(checkpoint, exp_dir / \"models\" / f'epoch_{epoch}.pt')\n",
    "\n",
    "    return best_acc, best_epoch\n",
    "\n",
    "def loadModelAttriburesAndTrainingAMP(\n",
    "    exp_dir, audio_model, image_model, attention, contrastive_loss, \n",
    "    optimizer, rank, last_not_best=True\n",
    "    ):\n",
    "\n",
    "    info_fn = exp_dir / \"training_metadata.json\"\n",
    "    with open(info_fn, \"r\") as f:\n",
    "        info = json.load(f)\n",
    "\n",
    "    if last_not_best:\n",
    "        checkpoint_fn = exp_dir / \"models\" / \"last_ckpt.pt\"\n",
    "    else:\n",
    "        checkpoint_fn = exp_dir / \"models\" / \"best_ckpt.pt\"\n",
    "\n",
    "    if rank != 'cuda':\n",
    "        checkpoint = torch.load(checkpoint_fn, map_location={'cuda:%d' % 0: 'cuda:%d' % rank})\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_fn)\n",
    "    \n",
    "    audio_model.load_state_dict(checkpoint[\"audio_model\"])\n",
    "    image_model.load_state_dict(checkpoint[\"image_model\"])\n",
    "    attention.load_state_dict(checkpoint[\"attention\"])\n",
    "    contrastive_loss.load_state_dict(checkpoint[\"contrastive_loss\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    # amp.load_state_dict(checkpoint[\"amp\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    global_step = checkpoint[\"global_step\"]\n",
    "    best_epoch = checkpoint[\"best_epoch\"]\n",
    "    best_acc = checkpoint[\"best_acc\"]  \n",
    "    \n",
    "    return info, epoch, global_step, best_epoch, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a272fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_boundaries_fn = Path('/storage/Datasets/flickr_audio/flickr_8k.ctm')\n",
    "flickr_audio_dir = flickr_boundaries_fn.parent / \"wavs\"\n",
    "flickr_images_fn = Path('/storage/Datasets/Flicker8k_Dataset/')\n",
    "flickr_segs_fn = Path('./data/flickr_image_masks/')\n",
    "\n",
    "config_library = {\n",
    "    \"multilingual\": \"English_Hindi_DAVEnet_config.json\",\n",
    "    \"multilingual+matchmap\": \"English_Hindi_matchmap_DAVEnet_config.json\",\n",
    "    \"english\": \"English_DAVEnet_config.json\",\n",
    "    \"english+matchmap\": \"English_matchmap_DAVEnet_config.json\",\n",
    "    \"hindi\": \"Hindi_DAVEnet_config.json\",\n",
    "    \"hindi+matchmap\": \"Hindi_matchmap_DAVEnet_config.json\",\n",
    "}\n",
    "\n",
    "scipy_windows = {\n",
    "    'hamming': scipy.signal.hamming,\n",
    "    'hann': scipy.signal.hann, \n",
    "    'blackman': scipy.signal.blackman,\n",
    "    'bartlett': scipy.signal.bartlett\n",
    "    }\n",
    "\n",
    "def myRandomCrop(im, resize, to_tensor):\n",
    "\n",
    "        im = resize(im)\n",
    "        im = to_tensor(im)\n",
    "        return im\n",
    "\n",
    "def preemphasis(signal,coeff=0.97):  \n",
    "    # function adapted from https://github.com/dharwath\n",
    "    \n",
    "    return np.append(signal[0],signal[1:]-coeff*signal[:-1])\n",
    "\n",
    "def LoadAudio(path, alignment, audio_conf):\n",
    "    threshold = 0\n",
    "    audio_type = audio_conf.get('audio_type')\n",
    "    if audio_type not in ['melspectrogram', 'spectrogram']:\n",
    "        raise ValueError('Invalid audio_type specified in audio_conf. Must be one of [melspectrogram, spectrogram]')\n",
    "\n",
    "    preemph_coef = audio_conf.get('preemph_coef')\n",
    "    sample_rate = audio_conf.get('sample_rate')\n",
    "    window_size = audio_conf.get('window_size')\n",
    "    window_stride = audio_conf.get('window_stride')\n",
    "    window_type = audio_conf.get('window_type')\n",
    "    num_mel_bins = audio_conf.get('num_mel_bins')\n",
    "    target_length = audio_conf.get('target_length')\n",
    "    fmin = audio_conf.get('fmin')\n",
    "    n_fft = audio_conf.get('n_fft', int(sample_rate * window_size))\n",
    "    win_length = int(sample_rate * window_size)\n",
    "    hop_length = int(sample_rate * window_stride)\n",
    "\n",
    "    # load audio, subtract DC, preemphasis\n",
    "    y, sr = librosa.load(path, sample_rate)\n",
    "    dur = librosa.get_duration(y=y, sr=sr)\n",
    "    nsamples = y.shape[0]\n",
    "    if y.size == 0:\n",
    "        y = np.zeros(target_length)\n",
    "    y = y - y.mean()\n",
    "    y = preemphasis(y, preemph_coef)\n",
    "\n",
    "    # compute mel spectrogram / filterbanks\n",
    "    stft = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "        window=scipy_windows.get(window_type, scipy_windows['hamming']))\n",
    "    spec = np.abs(stft)**2 # Power spectrum\n",
    "    if audio_type == 'melspectrogram':\n",
    "        mel_basis = librosa.filters.mel(sr, n_fft, n_mels=num_mel_bins, fmin=fmin)\n",
    "        melspec = np.dot(mel_basis, spec)\n",
    "        logspec = librosa.power_to_db(melspec, ref=np.max)\n",
    "    elif audio_type == 'spectrogram':\n",
    "        logspec = librosa.power_to_db(spec, ref=np.max)\n",
    "    # n_frames = logspec.shape[1]\n",
    "    logspec = torch.FloatTensor(logspec[:, np.maximum(alignment[0]- threshold, 0): np.minimum(alignment[1] + threshold, nsamples)])\n",
    "    nsamples = logspec.size(1)\n",
    "\n",
    "    return torch.tensor(logspec), nsamples#, n_frames\n",
    "\n",
    "def LoadImage(impath, resize, image_normalize, to_tensor):\n",
    "    img = Image.open(impath).convert('RGB')\n",
    "    # img = self.image_resize_and_crop(img)\n",
    "    img = myRandomCrop(img, resize, to_tensor)\n",
    "    img = image_normalize(img)\n",
    "    return img\n",
    "\n",
    "def PadFeat(feat, target_length, padval):\n",
    "    nframes = feat.shape[1]\n",
    "    pad = target_length - nframes\n",
    "\n",
    "    if pad > 0:\n",
    "        feat = np.pad(feat, ((0, 0), (0, pad)), 'constant',\n",
    "            constant_values=(padval, padval))\n",
    "    elif pad < 0:\n",
    "        nframes = target_length\n",
    "        feat = feat[:, 0: pad]\n",
    "\n",
    "    return torch.tensor(feat).unsqueeze(0), torch.tensor(nframes).unsqueeze(0)\n",
    "\n",
    "def get_detection_metric_count(hyp_trn, gt_trn):\n",
    "    # Get the number of true positive (n_tp), true positive + false positive (n_tp_fp) and true positive + false negative (n_tp_fn) for a one sample on the detection task\n",
    "    correct_tokens = set([token for token in gt_trn if token in hyp_trn])\n",
    "    n_tp = len(correct_tokens)\n",
    "    n_tp_fp = len(hyp_trn)\n",
    "    n_tp_fn = len(set(gt_trn))\n",
    "\n",
    "    return n_tp, n_tp_fp, n_tp_fn\n",
    "\n",
    "def eval_detection_prf(n_tp, n_tp_fp, n_tp_fn):\n",
    "    precision = n_tp / n_tp_fp\n",
    "    recall = n_tp / n_tp_fn\n",
    "    fscore = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, fscore\n",
    "\n",
    "def eval_detection_accuracy(hyp_loc, gt_loc):\n",
    "    score = 0\n",
    "    total = 0\n",
    "\n",
    "    for gt_start_end_frame, gt_token in gt_loc:\n",
    "    \n",
    "        if gt_token in [hyp_token for _, hyp_token in hyp_loc]:\n",
    "            score += 1\n",
    "        total += 1\n",
    "\n",
    "    return score, total\n",
    "\n",
    "def get_localisation_metric_count(hyp_loc, gt_loc):\n",
    "    # Get the number of true positive (n_tp), true positive + false positive (n_tp_fp) and true positive + false negative (n_tp_fn) for a one sample on the localisation task\n",
    "    n_tp = 0\n",
    "    n_fp = 0\n",
    "    n_fn = 0\n",
    "\n",
    "    for hyp_frame, hyp_token in hyp_loc:\n",
    "        if hyp_token not in [gt_token for _, gt_token in gt_loc]:\n",
    "            n_fp += 1\n",
    "\n",
    "    for gt_start_end_frame, gt_token in gt_loc:\n",
    "        if gt_token not in [hyp_token for _, hyp_token in hyp_loc]:\n",
    "            n_fn += 1\n",
    "            continue\n",
    "        for hyp_frame, hyp_token in hyp_loc:\n",
    "            if hyp_token == gt_token and (gt_start_end_frame[0] <= hyp_frame < gt_start_end_frame[1] or gt_start_end_frame[0] < hyp_frame <= gt_start_end_frame[1]):\n",
    "                n_tp += 1\n",
    "            elif hyp_token == gt_token and (hyp_frame < gt_start_end_frame[0] or gt_start_end_frame[1] < hyp_frame):\n",
    "                n_fp += 1\n",
    "\n",
    "\n",
    "    return n_tp, n_fp, n_fn\n",
    "\n",
    "def eval_localisation_accuracy(hyp_loc, gt_loc):\n",
    "    score = 0\n",
    "    total = 0\n",
    "\n",
    "    for gt_start_end_frame, gt_token in gt_loc:\n",
    "        if gt_token not in [hyp_token for _, hyp_token in hyp_loc]:\n",
    "            total += 1\n",
    "    \n",
    "        if gt_token in [hyp_token for _, hyp_token in hyp_loc]:\n",
    "            total += 1\n",
    "        \n",
    "        for hyp_frame, hyp_token in hyp_loc:\n",
    "            if hyp_token == gt_token and (gt_start_end_frame[0] <= hyp_frame < gt_start_end_frame[1] or gt_start_end_frame[0] < hyp_frame <= gt_start_end_frame[1]):\n",
    "                score += 1\n",
    "\n",
    "    return score, total\n",
    "\n",
    "def eval_localisation_prf(n_tp, n_fp, n_fn):\n",
    "    precision = n_tp / (n_tp + n_fp)\n",
    "    recall = n_tp / (n_tp + n_fn)\n",
    "    fscore = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, fscore\n",
    "\n",
    "def get_gt_token_duration(target_dur, valid_gt_trn):\n",
    "            \n",
    "    token_dur = []\n",
    "    for start_end, dur, tok in target_dur:\n",
    "        if tok not in valid_gt_trn:\n",
    "            continue\n",
    "        token_dur.append((start_end, tok.casefold()))\n",
    "    return token_dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ffa452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/params.json\n",
      "\n",
      "Recovering model arguments from\n",
      "     model_metadata\n",
      "       ↪ spokencoco_train\n",
      "        ↪ AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-2560499dfc_ConfigFile-params\n",
      "         ↪ args.pkl\n",
      "\n",
      "\n",
      "/home/leannenortje/10-shot_5-way/model_metadata/spokencoco_train/AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-2560499dfc_ConfigFile-params/args.pkl\n",
      "Model arguments:\n",
      "\tacoustic_model:\n",
      "\t\tin_channels: 40\n",
      "\t\tkernel_size: 4\n",
      "\t\tout_channels: 512\n",
      "\t\tpadding: 1\n",
      "\t\tstride: 2\n",
      "\n",
      "\n",
      "\talphas: [1.5, 1.2, 1.5, 1.2, 2.5, 2.5]\n",
      "\taudio_config:\n",
      "\t\taudio_type: melspectrogram\n",
      "\t\tfmin: 20\n",
      "\t\tnum_mel_bins: 40\n",
      "\t\tpadval: 0\n",
      "\t\tpreemph_coef: 0.97\n",
      "\t\tsample_rate: 16000\n",
      "\t\ttarget_length: 1024\n",
      "\t\tuse_raw_length: False\n",
      "\t\twindow_size: 0.025\n",
      "\t\twindow_stride: 0.01\n",
      "\t\twindow_type: hamming\n",
      "\n",
      "\n",
      "\taudio_model:\n",
      "\t\tc_dim: 512\n",
      "\t\tembedding_dim: 2048\n",
      "\t\tname: Transformer\n",
      "\t\tnum_heads: 8\n",
      "\t\tz_dim: 64\n",
      "\n",
      "\n",
      "\tbatch_size: 8\n",
      "\tcpc:\n",
      "\t\thop_length: 160\n",
      "\t\tload_pretrained_weights: True\n",
      "\t\tn_negatives: 17\n",
      "\t\tn_prediction_steps: 6\n",
      "\t\tn_sample_frames: 128\n",
      "\t\tn_speakers_per_batch: 4\n",
      "\t\tn_utterances_per_speaker: 8\n",
      "\t\tpretrained_weights: epoch_1500\n",
      "\t\twarm_start: True\n",
      "\n",
      "\n",
      "\tdata_path: data/spokencoco\n",
      "\tdata_test: data/novel_points.json\n",
      "\tdata_train: data/spokencoco_train.json\n",
      "\tdata_val: data/spokencoco_val.json\n",
      "\tepisodes_test: data/test_episodes.npz\n",
      "\texp_dir: model_metadata/spokencoco_train/AudioModel-Transformer_ImageModel-Resnet50_ArgumentsHash-2560499dfc_ConfigFile-params\n",
      "\timage_config:\n",
      "\t\tRGB_mean: [0.485, 0.456, 0.406]\n",
      "\t\tRGB_std: [0.229, 0.224, 0.225]\n",
      "\t\tcenter_crop: False\n",
      "\t\tcrop_size: 224\n",
      "\n",
      "\n",
      "\timage_model: Resnet50\n",
      "\tlearning_rate_scheduler:\n",
      "\t\tdecay_every_n_epochs: 10\n",
      "\t\tdecay_factor: 0.95\n",
      "\t\tinitial_learning_rate: 1e-05\n",
      "\t\tlearning_rates: [1e-05, 1e-05, 1e-05, 1e-05]\n",
      "\t\tnum_epochs: [30, 50, 70, 80]\n",
      "\n",
      "\n",
      "\tloss: matchmap\n",
      "\tmargin: 1.0\n",
      "\tmodel_name: 2560499dfc\n",
      "\tmomentum: 0.9\n",
      "\tn_epochs: 100\n",
      "\toptimizer: adam\n",
      "\tpretrained_image_model: True\n",
      "\tresume: False\n",
      "\tsimtype: MISA\n",
      "\tweight_decay: 5e-07\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): ScoringAttentionModule(\n",
      "    (audio_encoder): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "    (image_encoder): Sequential(\n",
      "      (0): Linear(in_features=49, out_features=49, bias=True)\n",
      "      (1): Linear(in_features=49, out_features=49, bias=True)\n",
      "    )\n",
      "    (pool_func): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (cos): CosineSimilarity()\n",
      "    (sig): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\", dest=\"resume\",\n",
    "        help=\"load from exp_dir if True\")\n",
    "parser.add_argument(\"--config-file\", type=str, default='matchmap', choices=['matchmap'], help=\"Model config file.\")\n",
    "parser.add_argument(\"--restore-epoch\", type=int, default=-1, help=\"Epoch to generate accuracies for.\")\n",
    "parser.add_argument(\"--image-base\", default=\"..\", help=\"Model config file.\")\n",
    "command_line_args = parser.parse_args(args=[])\n",
    "restore_epoch = command_line_args.restore_epoch\n",
    "\n",
    "# Setting up model specifics\n",
    "args, image_base = modelSetup(command_line_args, True)\n",
    "rank = 'cuda'\n",
    " \n",
    "concepts = []\n",
    "with open('./data/test_keywords.txt', 'r') as f:\n",
    "    for keyword in f:\n",
    "        concepts.append(keyword.strip())\n",
    "\n",
    "alignments = {}\n",
    "prev = ''\n",
    "prev_wav = ''\n",
    "prev_start = 0\n",
    "with open(Path('../Datasets/spokencoco/SpokenCOCO/words.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        wav, start, stop, label = line.strip().split()\n",
    "        if label in concepts or (label == 'hydrant' and prev == 'fire' and wav == prev_wav):\n",
    "            if wav not in alignments: alignments[wav] = {}\n",
    "            if label == 'hydrant' and prev == 'fire': \n",
    "                label = prev + \" \" + label\n",
    "                start = prev_start\n",
    "            if label not in alignments[wav]: alignments[wav][label] = (int(float(start)*100), int(float(stop)*100))\n",
    "        prev = label\n",
    "        prev_wav = wav\n",
    "        prev_start = start\n",
    "\n",
    "audio_conf = args[\"audio_config\"]\n",
    "target_length = audio_conf.get('target_length', 1024)\n",
    "padval = audio_conf.get('padval', 0)\n",
    "image_conf = args[\"image_config\"]\n",
    "crop_size = image_conf.get('crop_size')\n",
    "center_crop = image_conf.get('center_crop')\n",
    "RGB_mean = image_conf.get('RGB_mean')\n",
    "RGB_std = image_conf.get('RGB_std')\n",
    "\n",
    "# image_resize_and_crop = transforms.Compose(\n",
    "#         [transforms.Resize(224), transforms.ToTensor()])\n",
    "resize = transforms.Resize((256, 256))\n",
    "to_tensor = transforms.ToTensor()\n",
    "image_normalize = transforms.Normalize(mean=RGB_mean, std=RGB_std)\n",
    "\n",
    "image_resize = transforms.transforms.Resize((256, 256))\n",
    "trans = transforms.ToPILImage()\n",
    "\n",
    "# Create models\n",
    "audio_model = mutlimodal(args).to(rank)\n",
    "\n",
    "seed_model = alexnet(pretrained=True)\n",
    "image_model = nn.Sequential(*list(seed_model.features.children()))\n",
    "\n",
    "last_layer_index = len(list(image_model.children()))\n",
    "image_model.add_module(str(last_layer_index),\n",
    "    nn.Conv2d(256, args[\"audio_model\"][\"embedding_dim\"], kernel_size=(3,3), stride=(1,1), padding=(1,1)))\n",
    "image_model = image_model.to(rank)\n",
    "\n",
    "attention = ScoringAttentionModule(args).to(rank)\n",
    "contrastive_loss = ContrastiveLoss(args).to(rank)\n",
    "\n",
    "model_with_params_to_update = {\n",
    "    \"audio_model\": audio_model,\n",
    "    \"attention\": attention,\n",
    "    \"contrastive_loss\": contrastive_loss,\n",
    "    \"image_model\": image_model\n",
    "    }\n",
    "model_to_freeze = {\n",
    "    }\n",
    "trainable_parameters = getParameters(model_with_params_to_update, model_to_freeze, args)\n",
    "\n",
    "if args[\"optimizer\"] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(\n",
    "        trainable_parameters, args[\"learning_rate_scheduler\"][\"initial_learning_rate\"],\n",
    "        momentum=args[\"momentum\"], weight_decay=args[\"weight_decay\"]\n",
    "        )\n",
    "elif args[\"optimizer\"] == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        trainable_parameters, args[\"learning_rate_scheduler\"][\"initial_learning_rate\"],\n",
    "        weight_decay=args[\"weight_decay\"]\n",
    "        )\n",
    "else:\n",
    "    raise ValueError('Optimizer %s is not supported' % args[\"optimizer\"])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "audio_model = DDP(audio_model, device_ids=[rank])\n",
    "image_model = DDP(image_model, device_ids=[rank])\n",
    "attention = DDP(attention, device_ids=[rank])\n",
    "print(attention)\n",
    "info, epoch, global_step, best_epoch, best_acc = loadModelAttriburesAndTrainingAMP(\n",
    "    args[\"exp_dir\"], audio_model, image_model, attention, contrastive_loss, optimizer, \n",
    "    rank, False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e1dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base = Path('../Datasets/spokencoco/')\n",
    "episodes = np.load(args[\"episodes_test\"], allow_pickle=True)['episodes'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2481e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    matching_set_images = []\n",
    "    matching_set_labels = []\n",
    "    im_used = set()\n",
    "    counting = {}\n",
    "    print(len(episodes['matching_set']))\n",
    "    for im in tqdm(episodes['matching_set']):\n",
    "\n",
    "        imgpath = image_base / im\n",
    "        this_image = LoadImage(imgpath, resize, image_normalize, to_tensor)\n",
    "        this_image_output = image_model(this_image.unsqueeze(0).to(rank))\n",
    "        this_image_output = this_image_output.view(this_image_output.size(0), this_image_output.size(1), -1).transpose(1, 2)\n",
    "        # this_image_output = this_image_output.mean(dim=1)\n",
    "        matching_set_images.append(this_image_output)\n",
    "        matching_set_labels.append(episodes['matching_set'][im])\n",
    "        \n",
    "        for w in list(episodes['matching_set'][im]):\n",
    "            if w not in concepts: continue\n",
    "            if w not in counting: counting[w] = 0\n",
    "            counting[w] += 1\n",
    "            # if counting[w] == 10: break\n",
    "\n",
    "    print(counting)\n",
    "\n",
    "    matching_set_images = torch.cat(matching_set_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c1260cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test number 1-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:20<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broccoli: 94/100=94.00%\n",
      "zebra: 93/100=93.00%\n",
      "fire hydrant: 53/100=53.00%\n",
      "sheep: 79/100=79.00%\n",
      "kite: 76/100=76.00%\n",
      "Overall: 395/500=0.79=79.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    acc = []\n",
    "    for i_test in range(1):\n",
    "        print(f'\\nTest number {i_test+1}-----------------------------------')\n",
    "        results = {}\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "        queries = {}\n",
    "        query_names = {}\n",
    "\n",
    "        episode_names = list(episodes.keys())\n",
    "        episode_names.remove('matching_set')\n",
    "\n",
    "        episode_names = np.random.choice(episode_names, 100, replace=False)\n",
    "\n",
    "        for episode_num in tqdm(episode_names):\n",
    "\n",
    "            episode = episodes[episode_num]\n",
    "            \n",
    "            m_images = []\n",
    "            m_labels = []\n",
    "            counting = {}\n",
    "\n",
    "            for w in episode['matching_set']:\n",
    "\n",
    "                imgpath = image_base / episode['matching_set'][w]\n",
    "                this_image = LoadImage(imgpath, resize, image_normalize, to_tensor)\n",
    "                this_image_output = image_model(this_image.unsqueeze(0).to(rank))\n",
    "                this_image_output = this_image_output.view(this_image_output.size(0), this_image_output.size(1), -1).transpose(1, 2)\n",
    "                # this_image_output = this_image_output.mean(dim=1)\n",
    "                m_images.append(this_image_output)\n",
    "                m_labels.append(w)\n",
    "\n",
    "            for w in list(episode['matching_set']):\n",
    "                if w not in concepts: continue\n",
    "                if w not in counting: counting[w] = 0\n",
    "                counting[w] += 1\n",
    "                # if counting[w] == 10: break\n",
    "\n",
    "            m_images = torch.cat(m_images, axis=0)\n",
    "    \n",
    "            for w in episode['queries']:\n",
    "                if w not in results: results[w] = {'correct': 0, 'total': 0}\n",
    "                wav, spkr = episode['queries'][w]\n",
    "\n",
    "                lookup = str(Path(wav).stem)\n",
    "                if lookup in alignments:\n",
    "                    if w in alignments[lookup]:\n",
    "\n",
    "                        this_english_audio_feat, this_english_nframes = LoadAudio(image_base / 'SpokenCOCO' / wav, alignments[lookup][w], audio_conf)\n",
    "                        this_english_audio_feat, this_english_nframes = PadFeat(this_english_audio_feat, target_length, padval)\n",
    "                        _, _, query = audio_model(this_english_audio_feat.to(rank))\n",
    "                        n_frames = NFrames(this_english_audio_feat, query, this_english_nframes) \n",
    "                        scores = attention.module.one_to_many_score(m_images, query, n_frames).squeeze()\n",
    "                        \n",
    "                        indices = torch.argsort(scores, descending=True)[0: counting[w]]\n",
    "                        for ind in range(counting[w]):\n",
    "                            if w in m_labels[indices[ind]]: \n",
    "                                results[w]['correct'] += 1\n",
    "                            results[w]['total'] += 1\n",
    "                \n",
    "                \n",
    "            \n",
    "        c = 0\n",
    "        t = 0\n",
    "        for w in results:\n",
    "            correct = results[w]['correct']\n",
    "            total = results[w]['total']\n",
    "            c += correct\n",
    "            t += total\n",
    "            percentage = 100*correct/total\n",
    "            print(f'{w}: {correct}/{total}={percentage:<.2f}%')\n",
    "        percentage = c/t\n",
    "        print(f'Overall: {c}/{t}={percentage}={100*percentage:<.2f}%')\n",
    "\n",
    "        acc.append(100*percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff31600",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bc9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803af414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbd157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329ecd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c953c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af945f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547cf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41df4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6fe89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6753bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760534a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18f80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f51e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c53b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000f7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c08dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c77e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84fdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a9f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3aebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8182427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937edb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb851e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acb347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95c6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b47e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a92d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13cd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b3136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a70268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b10b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccf74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc53f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce279c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b4cef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5e8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee97f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6fa4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd395c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9856996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9142e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
